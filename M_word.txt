------------------------------------------------------------------------------------------------------------------------------------
## in such a format (ready to copy)

## structurally,
1. play
2. google search
3. content
4. play
5. reminder
6. youtube search
7. general
8. system
------------------------------------------------------------------------------------------------------------------------------------


‚úÖ What smart_hf_tokenizer.py script does now:

        üß† smart_hf_tokenizer.py ‚Äî Step-by-Step Overview
        1. Load Resources & Configuration
        
            CSV Dataset: Loads utterances + intent labels from Day2_cleaned_dataset.csv.
        
            Slang Mapping: Loads a dictionary of slang-to-formal mappings from SLANG_ABBREVIATIONS.txt.
        
            spaCy Model: Loads the transformer-based en_core_web_trf for dependency parsing.
        
        2. Normalize & Tokenize Utterances
        
        If hf_tokenizer_corpus.txt does not already exist:
        
            Normalize slang (e.g., "u" ‚Üí "you") using my loaded slang mapping.
        
            Use spaCy to tokenize and analyze each utterance.
        
            Apply custom token logic:
        
                Preserve phrasal verb roots (e.g., shut, log, turn if they are ROOT).
        
                Retain prepositions (e.g., down, off, out).
        
                Include all other tokens from the sentence.
        
            Save the final space-separated token strings (pre-tokenized utterances) to hf_tokenizer_corpus.txt.
        
            ‚úÖ If the file already exists, this entire step is skipped to save time.
        
        3. Train Hugging Face BPE Tokenizer
        
        If hf_tokenizer.json does not already exist:
        
            Initialize a Byte Pair Encoding (BPE) tokenizer with [UNK] as the unknown token.
        
            Use a whitespace pre-tokenizer (since the corpus is already tokenized).
        
            Train the tokenizer using Hugging Face‚Äôs BpeTrainer:
        
                Vocabulary size: 10,000
        
                Adds special tokens: [UNK], [PAD], [CLS], [SEP], [MASK]
        
            Save the trained tokenizer to hf_tokenizer.json.
        
            ‚úÖ If the tokenizer already exists, this step is skipped too.
        
        4. Optional Cleanup
        
        The temporary corpus file can be deleted after training, but this is currently disabled so you can reuse it later.



------------------------------------------------------------------------------------------------------------------------------------

SLANG_ABBREVIATIONS = {
    # üì± Social Media & Messaging Platforms
    "insta": "instagram",
    "fb": "facebook",
    "wp": "whatsapp",
    "tg": "telegram",
    "yt": "youtube",
    "tw": "twitter",
    "sc": "snapchat",
    "tt": "tiktok",
    "li": "linkedin",
    "pin": "pinterest",
    "dc": "discord",
    "a\c": "According to"
    "acc": "Account"

    # üí¨ General Chat Abbreviations
    "e.g.": "example",
    "e.g": "example",
    "ex": "example",
    "u": "you",
    "ur": "your",
    "r": "are",
    "b4": "before",
    "gr8": "great",
    "l8r": "later",
    "bday": "birthday",
    "idk": "i do not know",
    "imo": "in my opinion",
    "imho": "in my humble opinion",
    "brb": "be right back",
    "ttyl": "talk to you later",
    "omg": "oh my god",
    "lol": "laughing out loud",
    "btw": "by the way",
    "afaik": "as far as i know",
    "fyi": "for your information",
    "smh": "shaking my head",
    "tbh": "to be honest",
    "irl": "in real life",
    "thx": "thanks",
    "ty": "thank you",
    "yw": "you are welcome",
    "wtf": "what the heck",  # softened
    "wth": "what the heck",
    "bc": "because",
    "bcz": "because",
    "cuz": "because",
    "plz": "please",
    "pls": "please",
    "omw": "on my way",
    "jk": "just kidding",
    "gg": "good game",
    "hf": "have fun",
    "np": "no problem",
    "wyd": "what are you doing",
    "hbu": "how about you",
    "ilu": "i love you",
    "ily": "i love you",
    "bff": "best friend forever"
}

------------------------------------------------------------------------------------------------------------------------------------

integrating "en-80k.txt"  i.e "symspellpy==6.9.0"

------------------------------------------------------------------------------------------------------------------------------------


## Goal: I want a tokenization technique, That must be works very well in all types of complex + realword words (including emails, urls and dot containing words).
like: word tokenization: ‚ÄúI am new in New Delhi‚Äù,  ‚Äú$20‚Äù, ‚Äú10.5 KM‚Äù,‚Äù1.6M‚Äù , ‚Äú10KM‚Äù, ‚Äúlet‚Äôs‚Äù, ‚ÄúU.S‚Äù, ‚ÄúU.S.A‚Äù , ‚ÄúDelhi!‚Äù, ‚ÄúPh.D‚Äù, ‚ÄúA.I‚Äù, ‚Äúranjitdax89@gmailcom‚Äù,  ‚Äúundoable‚Äù ‚Üí ‚Äòun‚Äô, ‚Äòdo‚Äô, ‚Äòable‚Äô

must be works well. this words are just an example, 

## prefarences: ‚ÄúI prepares lemmatization more than stemming‚Äù & ‚Äúpos=‚Äôv‚Äô ‚Äú |parts of speech = verb  

## I don't care how muct time takes.  (regardless of time taken to do the tokenization, I'm thinking to use high level this like: BERT, GPT algorithms)

## work flow: 
1. It reads a dataset (.csv file) | pd.read_csv(r"/home/ranjit/Desktop/Decision_Making_Model/Intent_Data_Acquisition_Pipeline_1/Cleaned_Intent_Dataset.csv")

2. Do the tokenization with smart rules.

3. Saves in a new csv file. | df.to_csv("Day2_cleaned_dataset.csv", index=False) print("‚úÖ Dataset saved as 'Day2_cleaned_dataset.csv'")

## No code, just tell me techniques


------------------------------------------------------------------------------------------------------------------------------------




------------------------------------------------------------------------------------------------------------------------------------